params:
- train
- --help
output:
  exit_code: 0
  stdout: |
    Usage: base-callback train [OPTIONS]

    Options:
      --distributed / --no-distributed
                                      If the learner is distributed.  [default: no-
                                      distributed]
      --fp16 / --no-fp16              Whether or not the floating-point precision of
                                      learner should be set to 16 bit.  [default:
                                      fp16]
      --output-dir PATH               The location of the output directory.
                                      [default: ./outputs]
      --pretrained / --no-pretrained  Whether or not to use the pretrained weights.
                                      [default: pretrained]
      --weight-decay FLOAT            The amount of weight decay. If None then it
                                      uses the default amount of weight decay in
                                      fastai.
      --model-name [|alexnet|densenet121|densenet161|densenet169|densenet201|efficientnet_b0|efficientnet_b1|efficientnet_b2|efficientnet_b3|efficientnet_b4|efficientnet_b5|efficientnet_b6|efficientnet_b7|googlenet|inception_v3|mnasnet0_5|mnasnet0_75|mnasnet1_0|mnasnet1_3|mobilenet_v2|mobilenet_v3_large|mobilenet_v3_small|regnet_x_16gf|regnet_x_1_6gf|regnet_x_32gf|regnet_x_3_2gf|regnet_x_400mf|regnet_x_800mf|regnet_x_8gf|regnet_y_16gf|regnet_y_1_6gf|regnet_y_32gf|regnet_y_3_2gf|regnet_y_400mf|regnet_y_800mf|regnet_y_8gf|resnet101|resnet152|resnet18|resnet34|resnet50|resnext101_32x8d|resnext50_32x4d|shufflenet_v2_x0_5|shufflenet_v2_x1_0|shufflenet_v2_x1_5|shufflenet_v2_x2_0|squeezenet1_0|squeezenet1_1|vgg11|vgg11_bn|vgg13|vgg13_bn|vgg16|vgg16_bn|vgg19|vgg19_bn|wide_resnet101_2|wide_resnet50_2]
                                      The name of a model architecture in
                                      torchvision.models (https://pytorch.org/vision
                                      /stable/models.html). If not given, then it is
                                      given by `default_model_name`
      --epochs INTEGER                The number of epochs.  [default: 20]
      --freeze-epochs INTEGER         The number of epochs to train when the learner
                                      is frozen and the last layer is trained by
                                      itself. Only if `fine_tune` is set on the app.
                                      [default: 3]
      --learning-rate FLOAT           The base learning rate (when fine tuning) or
                                      the max learning rate otherwise.  [default:
                                      0.0001]
      --project-name TEXT             The name for this project for logging
                                      purposes.
      --run-name TEXT                 The name for this particular run for logging
                                      purposes.
      --run-id TEXT                   A unique ID for this particular run for
                                      logging purposes.
      --notes TEXT                    A longer description of the run for logging
                                      purposes.
      --tag TEXT                      A tag for logging purposes. Multiple tags can
                                      be added each introduced with --tag.
      --wandb / --no-wandb            Whether or not to use 'Weights and Biases' for
                                      logging.  [default: no-wandb]
      --wandb-mode TEXT               The mode for 'Weights and Biases'.  [default:
                                      online]
      --wandb-dir PATH                The location for 'Weights and Biases' output.
      --wandb-entity TEXT             An entity is a username or team name where
                                      you're sending runs.
      --wandb-group TEXT              Specify a group to organize individual runs
                                      into a larger experiment.
      --wandb-job-type TEXT           Specify the type of run, which is useful when
                                      you're grouping runs together into larger
                                      experiments using group.
      --mlflow / --no-mlflow          Whether or not to use MLflow for logging.
                                      [default: no-mlflow]
      --help                          Show this message and exit.
